{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '/home/paperspace/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/' does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing AsyncEngineArgs\n",
      "INFO:root:Initializing AsyncLLMEngine\n",
      "/home/paperspace/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29883b96e4784afc875382926b6b7f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 02:55:10 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528b0e4880f64831ad7758937adcaa49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b428e2ca3644b4bef2e4674cfedbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016a6a12c533480f93ba9db41b1129a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027d64630d834661be33d2f33d576e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 02:55:12 weight_utils.py:207] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5843d35b77b646d691f67a0665b1c4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed69ab89f6b840f7800e89050bbb7fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136c8a56fe314f27af09e59e107c2998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091adfc0a7c545a5acedccbd7abb29bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c0d4346cd84110a0b4b0baaac8420d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 02:55:39 model_runner.py:146] Loading model weights took 14.9595 GB\n",
      "INFO 06-22 02:55:40 gpu_executor.py:83] # GPU blocks: 27975, # CPU blocks: 2048\n",
      "INFO 06-22 02:55:42 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-22 02:55:42 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-22 02:55:46 model_runner.py:924] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Putting elapsed time in queue\n",
      "INFO:root:Exiting process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '/home/paperspace/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/' has been deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing AsyncEngineArgs\n",
      "INFO:root:Initializing AsyncLLMEngine\n",
      "/home/paperspace/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c2aa3dd55847c7a94baef7070a7845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 02:55:51 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0e89c0f55a44778f71b6d482b458de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa683fd65cd4ae5bd27521ea7084dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec39b7290ca74fd9b2d9295390d9b592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5faa19fc19548a1b99608f4809d847e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 02:55:53 weight_utils.py:207] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3563ac56759d4b55b8f5993d25e032dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bea51f2ab9945d3859b786e71023821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41da162727ba40d388cf077844385b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13caf28d0ca3478681c4bc38c83aa2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77587011dc04fd4943bdba7667e5b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 02:56:19 model_runner.py:146] Loading model weights took 14.9595 GB\n",
      "INFO 06-22 02:56:20 gpu_executor.py:83] # GPU blocks: 27975, # CPU blocks: 2048\n",
      "INFO 06-22 02:56:22 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-22 02:56:22 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-22 02:56:26 model_runner.py:924] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Putting elapsed time in queue\n",
      "INFO:root:Exiting process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '/home/paperspace/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/' has been deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing AsyncEngineArgs\n",
      "INFO:root:Initializing AsyncLLMEngine\n",
      "/home/paperspace/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9225384685c400eb710291781cf397c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 02:56:31 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156364763de44055bcec4c545771e672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb7cdae00f24e3ea36237274fbae8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b897219d88ce42b38dd2167c96627c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5275aae83cf74a2f91334b6cced840a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 02:56:33 weight_utils.py:207] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20fe2cedb9249e1bf5b55c76aac0ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9a3ece6f10492e99b5cfaadabdc43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4072cccc088f461dac24a839897c9a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5e0d386476472a902d7051378982c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b706845a94436684fd73579c48adf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 02:56:57 model_runner.py:146] Loading model weights took 14.9595 GB\n",
      "INFO 06-22 02:56:58 gpu_executor.py:83] # GPU blocks: 27975, # CPU blocks: 2048\n",
      "INFO 06-22 02:56:59 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-22 02:56:59 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-22 02:57:04 model_runner.py:924] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Putting elapsed time in queue\n",
      "INFO:root:Exiting process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 1: 39.10 seconds\n",
      "Elapsed time 2: 36.56 seconds\n",
      "Elapsed time 3: 34.46 seconds\n",
      "Average elapsed time: 36.71 seconds\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "def clear_cache():\n",
    "    folder_path = os.path.expanduser(\n",
    "        \"~/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/\"\n",
    "    )\n",
    "\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"Folder '{folder_path}' has been deleted.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' does not exist.\")\n",
    "\n",
    "def load_non_tensorize(queue):\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    try:\n",
    "        t1 = time.perf_counter()\n",
    "        from vllm import AsyncEngineArgs, AsyncLLMEngine\n",
    "\n",
    "        logging.info(\"Initializing AsyncEngineArgs\")\n",
    "        ENGINE_ARGS = AsyncEngineArgs(\n",
    "            model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        )\n",
    "\n",
    "        logging.info(\"Initializing AsyncLLMEngine\")\n",
    "        engine = AsyncLLMEngine.from_engine_args(ENGINE_ARGS)\n",
    "\n",
    "        t2 = time.perf_counter()\n",
    "        elapsed_time = t2 - t1\n",
    "        queue.put(elapsed_time)\n",
    "        logging.info(\"Putting elapsed time in queue\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in load_non_tensorize: {e}\")\n",
    "    finally:\n",
    "        logging.info(\"Exiting process\")\n",
    "        os._exit(0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    elapsed_times = []\n",
    "\n",
    "    for i in range(3):\n",
    "        queue = multiprocessing.Queue()\n",
    "        clear_cache()\n",
    "        process = multiprocessing.Process(target=load_non_tensorize, args=(queue,))\n",
    "        process.start()\n",
    "        process.join()\n",
    "\n",
    "        if not queue.empty():\n",
    "            elapsed_time = queue.get()\n",
    "            elapsed_times.append(elapsed_time)\n",
    "        else:\n",
    "            print(\"Queue is empty, process might have failed.\")\n",
    "\n",
    "    for i, elapsed_time in enumerate(elapsed_times):\n",
    "        print(f\"Elapsed time {i + 1}: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    if elapsed_times:\n",
    "        print(f\"Average elapsed time: {sum(elapsed_times) / len(elapsed_times):.2f} seconds\")\n",
    "    else:\n",
    "        print(\"No elapsed times recorded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing AsyncEngineArgs\n",
      "INFO:root:Initializing AsyncLLMEngine\n",
      "/home/paperspace/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 03:09:19 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.TENSORIZER, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 03:09:26 tensorizer.py:344] Deserialized 16.1 GB in 5.48s, 2.9 GB/s\n",
      "INFO 06-22 03:09:26 tensorizer.py:346] Memory usage before: CPU: (maxrss: 1,592MiB F: 9,252MiB) GPU: (U: 814MiB F: 80,414MiB T: 81,228MiB) TORCH: (R: 22MiB/22MiB, A: 2MiB/10MiB)\n",
      "INFO 06-22 03:09:26 tensorizer.py:347] Memory usage after: CPU: (maxrss: 6,457MiB F: 5,406MiB) GPU: (U: 16,172MiB F: 65,056MiB T: 81,228MiB) TORCH: (R: 15,374MiB/15,374MiB, A: 15,318MiB/15,321MiB)\n",
      "INFO 06-22 03:09:26 model_runner.py:146] Loading model weights took 14.9595 GB\n",
      "INFO 06-22 03:09:27 gpu_executor.py:83] # GPU blocks: 27954, # CPU blocks: 2048\n",
      "INFO 06-22 03:09:28 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-22 03:09:28 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-22 03:09:33 model_runner.py:924] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Putting elapsed time in queue\n",
      "INFO:root:Exiting process\n",
      "INFO:root:Initializing AsyncEngineArgs\n",
      "INFO:root:Initializing AsyncLLMEngine\n",
      "/home/paperspace/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 03:09:35 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.TENSORIZER, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 03:09:41 tensorizer.py:344] Deserialized 16.1 GB in 5.37s, 3.0 GB/s\n",
      "INFO 06-22 03:09:41 tensorizer.py:346] Memory usage before: CPU: (maxrss: 1,591MiB F: 9,271MiB) GPU: (U: 814MiB F: 80,414MiB T: 81,228MiB) TORCH: (R: 22MiB/22MiB, A: 2MiB/10MiB)\n",
      "INFO 06-22 03:09:41 tensorizer.py:347] Memory usage after: CPU: (maxrss: 6,456MiB F: 5,400MiB) GPU: (U: 16,172MiB F: 65,056MiB T: 81,228MiB) TORCH: (R: 15,374MiB/15,374MiB, A: 15,318MiB/15,321MiB)\n",
      "INFO 06-22 03:09:42 model_runner.py:146] Loading model weights took 14.9595 GB\n",
      "INFO 06-22 03:09:43 gpu_executor.py:83] # GPU blocks: 27954, # CPU blocks: 2048\n",
      "INFO 06-22 03:09:44 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-22 03:09:44 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-22 03:09:49 model_runner.py:924] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Putting elapsed time in queue\n",
      "INFO:root:Exiting process\n",
      "INFO:root:Initializing AsyncEngineArgs\n",
      "INFO:root:Initializing AsyncLLMEngine\n",
      "/home/paperspace/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 03:09:50 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.TENSORIZER, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 03:09:57 tensorizer.py:344] Deserialized 16.1 GB in 5.42s, 3.0 GB/s\n",
      "INFO 06-22 03:09:57 tensorizer.py:346] Memory usage before: CPU: (maxrss: 1,592MiB F: 9,274MiB) GPU: (U: 814MiB F: 80,414MiB T: 81,228MiB) TORCH: (R: 22MiB/22MiB, A: 2MiB/10MiB)\n",
      "INFO 06-22 03:09:57 tensorizer.py:347] Memory usage after: CPU: (maxrss: 6,457MiB F: 5,431MiB) GPU: (U: 16,172MiB F: 65,056MiB T: 81,228MiB) TORCH: (R: 15,374MiB/15,374MiB, A: 15,318MiB/15,321MiB)\n",
      "INFO 06-22 03:09:57 model_runner.py:146] Loading model weights took 14.9595 GB\n",
      "INFO 06-22 03:09:58 gpu_executor.py:83] # GPU blocks: 27954, # CPU blocks: 2048\n",
      "INFO 06-22 03:09:59 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-22 03:09:59 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-22 03:10:04 model_runner.py:924] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Putting elapsed time in queue\n",
      "INFO:root:Exiting process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 1: 14.92 seconds\n",
      "Elapsed time 2: 14.09 seconds\n",
      "Elapsed time 3: 14.09 seconds\n",
      "Average elapsed time: 14.37 seconds\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import traceback\n",
    "from vllm.model_executor.model_loader.tensorizer import TensorizerConfig\n",
    "from tensorizer import stream_io\n",
    "\n",
    "stream_io._ensure_https_endpoint = lambda x: x\n",
    "\n",
    "\n",
    "def load_tensorize(queue):\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    try:\n",
    "        t1 = time.perf_counter()\n",
    "        from vllm import AsyncEngineArgs, AsyncLLMEngine\n",
    "        logging.info(\"Initializing AsyncEngineArgs\")\n",
    "        ENGINE_ARGS = AsyncEngineArgs(\n",
    "            load_format=\"tensorizer\",\n",
    "            model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            model_loader_extra_config=TensorizerConfig(\n",
    "                tensorizer_uri=\"s3://bentoml-s3-store/vllm/meta-llama/Meta-Llama-3-8B-Instruct/v1/model.tensors\",\n",
    "                num_readers=8,\n",
    "                s3_endpoint=\"http://127.0.0.1:9000\",\n",
    "                s3_access_key_id=\"otLXqhmx6GBR8nNxLwJx\",\n",
    "                s3_secret_access_key=\"THLw14CUoi9oNz12J31jUdO02amxzHtzNxVLnsP2\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        logging.info(\"Initializing AsyncLLMEngine\")\n",
    "        engine = AsyncLLMEngine.from_engine_args(ENGINE_ARGS)\n",
    "\n",
    "        t2 = time.perf_counter()\n",
    "        elapsed_time = t2 - t1\n",
    "        queue.put(elapsed_time)\n",
    "        logging.info(\"Putting elapsed time in queue\")\n",
    "    except Exception as e:\n",
    "                \n",
    "        logging.error(f\"Error in load_tensorize: {e}\")\n",
    "    finally:\n",
    "        logging.info(\"Exiting process\")\n",
    "        os._exit(0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    elapsed_times = []\n",
    "\n",
    "    for i in range(3):\n",
    "        queue = multiprocessing.Queue()\n",
    "        process = multiprocessing.Process(target=load_tensorize, args=(queue,))\n",
    "        process.start()\n",
    "        process.join()\n",
    "\n",
    "        if not queue.empty():\n",
    "            elapsed_time = queue.get()\n",
    "            elapsed_times.append(elapsed_time)\n",
    "        else:\n",
    "            print(\"Queue is empty, process might have failed.\")\n",
    "\n",
    "    for i, elapsed_time in enumerate(elapsed_times):\n",
    "        print(f\"Elapsed time {i + 1}: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    if elapsed_times:\n",
    "        print(f\"Average elapsed time: {sum(elapsed_times) / len(elapsed_times):.2f} seconds\")\n",
    "    else:\n",
    "        print(\"No elapsed times recorded.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bento",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
