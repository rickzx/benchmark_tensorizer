{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test vLLM Model Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. With Tensorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing AsyncEngineArgs\n",
      "INFO:root:Initializing AsyncLLMEngine\n",
      "/home/paperspace/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-23 10:25:23 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='EleutherAI/gpt-j-6B', speculative_config=None, tokenizer='EleutherAI/gpt-j-6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float32, max_seq_len=2048, download_dir=None, load_format=LoadFormat.TENSORIZER, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=EleutherAI/gpt-j-6B)\n",
      "INFO 06-23 10:25:23 selector.py:125] Cannot use FlashAttention-2 backend for dtype other than torch.float16 or torch.bfloat16.\n",
      "INFO 06-23 10:25:23 selector.py:51] Using XFormers backend.\n",
      "Downloading https://accel-object.ord1.coreweave.com/tensorized/EleutherAI/gpt-j-6B/model.tensors\n",
      "INFO 06-23 10:25:25 selector.py:125] Cannot use FlashAttention-2 backend for dtype other than torch.float16 or torch.bfloat16.\n",
      "INFO 06-23 10:25:25 selector.py:51] Using XFormers backend.\n",
      "WARNING 06-23 10:25:25 tensorizer.py:358] Deserializing HuggingFace models is not optimized for loading on vLLM, as tensorizer is forced to load to CPU. Consider deserializing a vLLM model instead for faster load times. See the examples/tensorize_vllm_model.py example script for serializing vLLM models.\n",
      "Downloading https://accel-object.ord1.coreweave.com/tensorized/EleutherAI/gpt-j-6B/model.tensors\n",
      "INFO 06-23 10:25:44 model_runner.py:146] Loading model weights took 22.5428 GB\n",
      "INFO 06-23 10:25:46 gpu_executor.py:83] # GPU blocks: 3503, # CPU blocks: 292\n",
      "INFO 06-23 10:25:49 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-23 10:25:49 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-23 10:25:58 model_runner.py:924] Graph capturing finished in 9 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Putting elapsed time in queue\n",
      "INFO:root:Exiting process\n",
      "INFO:root:Initializing AsyncEngineArgs\n",
      "INFO:root:Initializing AsyncLLMEngine\n",
      "/home/paperspace/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-23 10:26:00 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='EleutherAI/gpt-j-6B', speculative_config=None, tokenizer='EleutherAI/gpt-j-6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float32, max_seq_len=2048, download_dir=None, load_format=LoadFormat.TENSORIZER, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=EleutherAI/gpt-j-6B)\n",
      "INFO 06-23 10:26:00 selector.py:125] Cannot use FlashAttention-2 backend for dtype other than torch.float16 or torch.bfloat16.\n",
      "INFO 06-23 10:26:00 selector.py:51] Using XFormers backend.\n",
      "Downloading https://accel-object.ord1.coreweave.com/tensorized/EleutherAI/gpt-j-6B/model.tensors\n",
      "INFO 06-23 10:26:02 selector.py:125] Cannot use FlashAttention-2 backend for dtype other than torch.float16 or torch.bfloat16.\n",
      "INFO 06-23 10:26:02 selector.py:51] Using XFormers backend.\n",
      "WARNING 06-23 10:26:02 tensorizer.py:358] Deserializing HuggingFace models is not optimized for loading on vLLM, as tensorizer is forced to load to CPU. Consider deserializing a vLLM model instead for faster load times. See the examples/tensorize_vllm_model.py example script for serializing vLLM models.\n",
      "Downloading https://accel-object.ord1.coreweave.com/tensorized/EleutherAI/gpt-j-6B/model.tensors\n",
      "INFO 06-23 10:26:19 model_runner.py:146] Loading model weights took 22.5428 GB\n",
      "INFO 06-23 10:26:21 gpu_executor.py:83] # GPU blocks: 3503, # CPU blocks: 292\n",
      "INFO 06-23 10:26:24 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-23 10:26:24 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-23 10:26:33 model_runner.py:924] Graph capturing finished in 9 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Putting elapsed time in queue\n",
      "INFO:root:Exiting process\n",
      "INFO:root:Initializing AsyncEngineArgs\n",
      "INFO:root:Initializing AsyncLLMEngine\n",
      "/home/paperspace/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-23 10:26:35 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='EleutherAI/gpt-j-6B', speculative_config=None, tokenizer='EleutherAI/gpt-j-6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float32, max_seq_len=2048, download_dir=None, load_format=LoadFormat.TENSORIZER, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=EleutherAI/gpt-j-6B)\n",
      "INFO 06-23 10:26:35 selector.py:125] Cannot use FlashAttention-2 backend for dtype other than torch.float16 or torch.bfloat16.\n",
      "INFO 06-23 10:26:35 selector.py:51] Using XFormers backend.\n",
      "Downloading https://accel-object.ord1.coreweave.com/tensorized/EleutherAI/gpt-j-6B/model.tensors\n",
      "INFO 06-23 10:26:37 selector.py:125] Cannot use FlashAttention-2 backend for dtype other than torch.float16 or torch.bfloat16.\n",
      "INFO 06-23 10:26:37 selector.py:51] Using XFormers backend.\n",
      "WARNING 06-23 10:26:37 tensorizer.py:358] Deserializing HuggingFace models is not optimized for loading on vLLM, as tensorizer is forced to load to CPU. Consider deserializing a vLLM model instead for faster load times. See the examples/tensorize_vllm_model.py example script for serializing vLLM models.\n",
      "Downloading https://accel-object.ord1.coreweave.com/tensorized/EleutherAI/gpt-j-6B/model.tensors\n",
      "INFO 06-23 10:26:55 model_runner.py:146] Loading model weights took 22.5428 GB\n",
      "INFO 06-23 10:26:57 gpu_executor.py:83] # GPU blocks: 3503, # CPU blocks: 292\n",
      "INFO 06-23 10:27:00 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-23 10:27:00 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-23 10:27:09 model_runner.py:924] Graph capturing finished in 9 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Putting elapsed time in queue\n",
      "INFO:root:Exiting process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 1: 35.19 seconds\n",
      "Elapsed time 2: 33.38 seconds\n",
      "Elapsed time 3: 34.08 seconds\n",
      "Average elapsed time: 34.22 seconds\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import traceback\n",
    "from vllm.model_executor.model_loader.tensorizer import TensorizerConfig\n",
    "from tensorizer import stream_io\n",
    "\n",
    "# stream_io._ensure_https_endpoint = lambda x: x\n",
    "\n",
    "\n",
    "def load_tensorize(queue):\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    try:\n",
    "        t1 = time.perf_counter()\n",
    "        from vllm import AsyncEngineArgs, AsyncLLMEngine\n",
    "        logging.info(\"Initializing AsyncEngineArgs\")\n",
    "        ENGINE_ARGS = AsyncEngineArgs(\n",
    "            load_format=\"tensorizer\",\n",
    "            model=\"EleutherAI/gpt-j-6B\",\n",
    "            dtype=\"float32\",\n",
    "            model_loader_extra_config=TensorizerConfig(\n",
    "                tensorizer_uri=\"s3://tensorized/EleutherAI/gpt-j-6B/model.tensors\",\n",
    "                num_readers=8,\n",
    "                s3_endpoint=\"https://accel-object.ord1.coreweave.com\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # ENGINE_ARGS = AsyncEngineArgs(\n",
    "        #     load_format=\"tensorizer\",\n",
    "        #     model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        #     model_loader_extra_config=TensorizerConfig(\n",
    "        #         tensorizer_uri=\"s3://bentoml-s3-store/vllm/meta-llama/Meta-Llama-3-8B-Instruct/v1/model.tensors\",\n",
    "        #         num_readers=4,\n",
    "        #         s3_endpoint=\"http://35.184.72.229:9000\",\n",
    "        #         s3_access_key_id=\"ViXQIKesbLbo3VEq8gYt\",\n",
    "        #         s3_secret_access_key=\"QOJS4ClovqsI7tyMvmI83xiopYrt8CUv7vd0567X\",\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "        logging.info(\"Initializing AsyncLLMEngine\")\n",
    "        engine = AsyncLLMEngine.from_engine_args(ENGINE_ARGS)\n",
    "\n",
    "        t2 = time.perf_counter()\n",
    "        elapsed_time = t2 - t1\n",
    "        queue.put(elapsed_time)\n",
    "        logging.info(\"Putting elapsed time in queue\")\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        logging.error(f\"Error in load_tensorize: {e}\")\n",
    "    finally:\n",
    "        logging.info(\"Exiting process\")\n",
    "        os._exit(0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    elapsed_times = []\n",
    "\n",
    "    for i in range(3):\n",
    "        queue = multiprocessing.Queue()\n",
    "        process = multiprocessing.Process(target=load_tensorize, args=(queue,))\n",
    "        process.start()\n",
    "        process.join()\n",
    "\n",
    "        if not queue.empty():\n",
    "            elapsed_time = queue.get()\n",
    "            elapsed_times.append(elapsed_time)\n",
    "        else:\n",
    "            print(\"Queue is empty, process might have failed.\")\n",
    "\n",
    "    for i, elapsed_time in enumerate(elapsed_times):\n",
    "        print(f\"Elapsed time {i + 1}: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    if elapsed_times:\n",
    "        print(f\"Average elapsed time: {sum(elapsed_times) / len(elapsed_times):.2f} seconds\")\n",
    "    else:\n",
    "        print(\"No elapsed times recorded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Without Tensorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '/home/paperspace/.cache/huggingface/hub/' has been deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing AsyncEngineArgs\n",
      "INFO:root:Initializing AsyncLLMEngine\n",
      "/home/paperspace/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36439e008684a968e6b1c491cc71e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/930 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-23 10:28:41 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='EleutherAI/gpt-j-6B', speculative_config=None, tokenizer='EleutherAI/gpt-j-6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float32, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=EleutherAI/gpt-j-6B)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4bc1da2d5945ff8f8f7740585d3440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c84bab0c0f48bf8d00624edf547a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3860b42bd7141ed978e7139d2b68b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8385b928184e058dc4c5ae9784210e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab7eba9aa1849f59b8b59c8629fda28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/4.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8f56a899474249bb8bdf73c54e51de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-23 10:28:43 selector.py:125] Cannot use FlashAttention-2 backend for dtype other than torch.float16 or torch.bfloat16.\n",
      "INFO 06-23 10:28:43 selector.py:51] Using XFormers backend.\n",
      "INFO 06-23 10:28:44 selector.py:125] Cannot use FlashAttention-2 backend for dtype other than torch.float16 or torch.bfloat16.\n",
      "INFO 06-23 10:28:44 selector.py:51] Using XFormers backend.\n",
      "INFO 06-23 10:28:45 weight_utils.py:207] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f66fa1294974394b3055a3e393ef1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/24.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-23 10:30:18 model_runner.py:146] Loading model weights took 22.5428 GB\n",
      "INFO 06-23 10:30:20 gpu_executor.py:83] # GPU blocks: 3503, # CPU blocks: 292\n",
      "INFO 06-23 10:30:23 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-23 10:30:23 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-23 10:30:31 model_runner.py:924] Graph capturing finished in 9 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Putting elapsed time in queue\n",
      "INFO:root:Exiting process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '/home/paperspace/.cache/huggingface/hub/' has been deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing AsyncEngineArgs\n",
      "INFO:root:Initializing AsyncLLMEngine\n",
      "/home/paperspace/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65820047e69a43379e01e65890dd0dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/930 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-23 10:30:36 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='EleutherAI/gpt-j-6B', speculative_config=None, tokenizer='EleutherAI/gpt-j-6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float32, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=EleutherAI/gpt-j-6B)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064e67a230554828a0612a0c562bfa94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c7d93b358841eda2cfe993c5c821aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46d1e93c9f344d0815c6e89f1e9dd9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0eee9fcdca4ccf84e298fca970f027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a113a5aece646febd42737167895036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/4.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac07027b2dc4d5eb357f96db6b65327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-23 10:30:37 selector.py:125] Cannot use FlashAttention-2 backend for dtype other than torch.float16 or torch.bfloat16.\n",
      "INFO 06-23 10:30:37 selector.py:51] Using XFormers backend.\n",
      "INFO 06-23 10:30:38 selector.py:125] Cannot use FlashAttention-2 backend for dtype other than torch.float16 or torch.bfloat16.\n",
      "INFO 06-23 10:30:38 selector.py:51] Using XFormers backend.\n",
      "INFO 06-23 10:30:39 weight_utils.py:207] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4097794c0a584f3cae3422ae06bd9273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/24.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-23 10:32:11 model_runner.py:146] Loading model weights took 22.5428 GB\n",
      "INFO 06-23 10:32:13 gpu_executor.py:83] # GPU blocks: 3503, # CPU blocks: 292\n",
      "INFO 06-23 10:32:16 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-23 10:32:16 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-23 10:32:25 model_runner.py:924] Graph capturing finished in 9 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Putting elapsed time in queue\n",
      "INFO:root:Exiting process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 1: 110.02 seconds\n",
      "Elapsed time 2: 109.17 seconds\n",
      "Average elapsed time: 109.60 seconds\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "def clear_cache():\n",
    "    folder_path = os.path.expanduser(\n",
    "        \"~/.cache/huggingface/hub/\"\n",
    "    )\n",
    "\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"Folder '{folder_path}' has been deleted.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' does not exist.\")\n",
    "\n",
    "def load_non_tensorize(queue):\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    try:\n",
    "        t1 = time.perf_counter()\n",
    "        from vllm import AsyncEngineArgs, AsyncLLMEngine\n",
    "\n",
    "        logging.info(\"Initializing AsyncEngineArgs\")\n",
    "        ENGINE_ARGS = AsyncEngineArgs(\n",
    "            model=\"EleutherAI/gpt-j-6B\",\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "\n",
    "        logging.info(\"Initializing AsyncLLMEngine\")\n",
    "        engine = AsyncLLMEngine.from_engine_args(ENGINE_ARGS)\n",
    "\n",
    "        t2 = time.perf_counter()\n",
    "        elapsed_time = t2 - t1\n",
    "        queue.put(elapsed_time)\n",
    "        logging.info(\"Putting elapsed time in queue\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in load_non_tensorize: {e}\")\n",
    "    finally:\n",
    "        logging.info(\"Exiting process\")\n",
    "        os._exit(0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    elapsed_times = []\n",
    "\n",
    "    for i in range(2):\n",
    "        queue = multiprocessing.Queue()\n",
    "        clear_cache()\n",
    "        process = multiprocessing.Process(target=load_non_tensorize, args=(queue,))\n",
    "        process.start()\n",
    "        process.join()\n",
    "\n",
    "        if not queue.empty():\n",
    "            elapsed_time = queue.get()\n",
    "            elapsed_times.append(elapsed_time)\n",
    "        else:\n",
    "            print(\"Queue is empty, process might have failed.\")\n",
    "\n",
    "    for i, elapsed_time in enumerate(elapsed_times):\n",
    "        print(f\"Elapsed time {i + 1}: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    if elapsed_times:\n",
    "        print(f\"Average elapsed time: {sum(elapsed_times) / len(elapsed_times):.2f} seconds\")\n",
    "    else:\n",
    "        print(\"No elapsed times recorded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Raw Huggingface AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. With Tensorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092276359fd445398ac52dc71716b03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deserializing to cuda:\n",
      "Downloading https://accel-object.ord1.coreweave.com/tensorized/EleutherAI/gpt-j-6B/model.tensors\n",
      "Deserialized 24.3 GB in 16.11s, 1.5 GB/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: I have a dream that one day, Moms and Dads will make some more peace in this world.\n",
      "\n",
      "Moms and Dads\n",
      "\n",
      "I have such a sweet family... and they ALL were awesome today. Even the kids got along and weren't throwing fits (except\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from tensorizer import TensorDeserializer\n",
    "from tensorizer.utils import no_init_or_tensor, convert_bytes, get_mem_usage\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "model_ref = \"EleutherAI/gpt-j-6B\"\n",
    "s3_uri = \"s3://tensorized/EleutherAI/gpt-j-6B/model.tensors\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_ref)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# This ensures that the pretrained model weights are not initialized,\n",
    "# and non-persistent buffers (generated at runtime) are on the correct device.\n",
    "\n",
    "start = time.perf_counter()\n",
    "with torch.device(device), no_init_or_tensor():\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "print(f\"Deserializing to {device}:\")\n",
    "before_mem = get_mem_usage()\n",
    "\n",
    "# Lazy load the tensors from S3 into the model.\n",
    "deserializer = TensorDeserializer(s3_uri, device=device, num_readers=8)\n",
    "deserializer.load_into_module(model)\n",
    "end = time.perf_counter()\n",
    "\n",
    "after_mem = get_mem_usage()\n",
    "\n",
    "# Brag about how fast we are.\n",
    "total_bytes_str = convert_bytes(deserializer.total_tensor_bytes)\n",
    "duration = end - start\n",
    "per_second = convert_bytes(deserializer.total_tensor_bytes / duration)\n",
    "deserializer.close()\n",
    "print(f\"Deserialized {total_bytes_str} in {end - start:0.2f}s, {per_second}/s\")\n",
    "\n",
    "# Tokenize and generate\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ref)\n",
    "eos = tokenizer.eos_token_id\n",
    "input_ids = tokenizer.encode(\n",
    "    \"I have a dream that one day\", return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids, max_new_tokens=50, do_sample=True, pad_token_id=eos\n",
    "    )\n",
    "\n",
    "print(f\"Output: {tokenizer.decode(output[0], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Without Tensorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c8c9ebbf5b4886aa678e36d4a87702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/24.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deserialized 24.2 GB in 79.38s, 304.9 MB/s\n",
      "Output: I have a dream that one day  \n",
      "they'll all be brothers.\n",
      "\n",
      "But then I have a dream  \n",
      "that I might be the first.\n",
      "\n",
      "Who's gonna hold my hand,  \n",
      "baby, and walk me over  \n",
      "that bridge?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_ref = \"EleutherAI/gpt-j-6B\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_ref, device_map=device)\n",
    "end = time.perf_counter()\n",
    "\n",
    "total_bytes_str = convert_bytes(model.num_parameters() * 4)\n",
    "duration = end - start\n",
    "per_second = convert_bytes(model.num_parameters() * 4 / duration)\n",
    "deserializer.close()\n",
    "print(f\"Deserialized {total_bytes_str} in {end - start:0.2f}s, {per_second}/s\")\n",
    "\n",
    "# Tokenize and generate\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ref)\n",
    "eos = tokenizer.eos_token_id\n",
    "input_ids = tokenizer.encode(\n",
    "    \"I have a dream that one day \", return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids, max_new_tokens=50, do_sample=True, pad_token_id=eos\n",
    "    )\n",
    "\n",
    "print(f\"Output: {tokenizer.decode(output[0], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bento",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
